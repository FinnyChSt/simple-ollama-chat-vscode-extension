# deep-ollama-chat README

This is a simple chat extension that allows you to chat with a local ollama server via vscode.
This respitory was inspired by a youtube video from [Fireship](https://www.youtube.com/watch?v=clJCDHml2cA&t=348s&ab_channel=BeyondFireship)

## Features

The main feature so far is to be able to chat with a locally run ollama server.
run the following command in in the command Palette (cmd+shift+p): Ollama-Chat to start the chat.

![CHAT](https://github.com/FinnyChSt/simple-ollama-chat-vscode-extension/blob/main/images/chat-screen.png)

## Requirements

You need to have a local ollama server running. You can find the server at the following link: [Ollama Server](https://ollama.com/)
Right now it runs with the deepseek-r1:8b which needs to be running on your local machine.
To run the server you can use the following command:

```bash
ollama serve
```

## Extension Settings

None so far.

## Known Issues

None so far.

## Release Notes

This are the following release notes

### 0.0.1

Initial release of deep-ollama-chat extension to chat with ollama server.
